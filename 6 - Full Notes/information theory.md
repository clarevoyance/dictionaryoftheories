2025-04-07 08:59

Status:
Tags: [[Economics]] [[Mathematics]] [[Physics]] [[Music]]

### Economics/Music/Physics
The mathematical study of information, its storage by codes (see also [[combinatorics]]) and its transmission through channels of limited capacity (see also [[communication theory]])

A fundamental idea is the *entropy* of a set of events, 
$$H=-\sum^n_{k=1}p_k\log p_k$$
where $p_k$'s are the probability of each event. This notion was introduced by C E Shannon in 1948 to measure the uncertainty of the set. *Channels* are considered as mechanisms which take a letter of an input *alphabet* and transmit letters of an output alphabet with various probabilities, depending on how prone they are to error or *noise*. Shannon was then able to measure the effectiveness of the channel, which he called *capacity*, using ergodic theory (see also [[Birkhoff's ergodic theorem]]) and entropy
### Music
Adapted from science by various music theorists, notably the American Leonard B Meyer (1918-), this term refers to the notion that music can be analyzed statistically as a chain of events.

Each event (note, chord) suggests a possible continuation. If the predicted event occurs then no more information is collected; if the predicted event does not occur then the new event is added to the index of possible continuations, thus modifying the criteria for future predictions. The completed survey yields an objective record of repetition (unity) and diversity in the musical syntax. Though accurate answers are guaranteed, the process relies on the integrity of its user in deciding what information is meaningful.
# References
- R C Pinkerton, 'Information Theory and Melody', *Scientific American* vol CXCIV (1956)